DONE

	When I packets all the same size it works, even if the size is something odd like 99.

	When I send varying packet sizes only a few get through and then it stops.

	I'm not really sure what is causing this? It seems very strange.

	Clue. It *also* breaks in a similar way when sending to multiple client IPs.

	It breaks the same way even if the client is on a linux server, so it's not something wrong with my ISP connection or the Mac dropping packets in some way.

	The NIC definitely thinks it has transferred the packets out and isn't triggering any errors, so it could be something wrong with how the packet is constructed that gets it dropped on the internet.

	But I can't understand how it would only break in this particular way?

	Disabling zero copy on the XDP driver makes no difference. If it was a driver issues, this probably would have fixed it.

	There is no problem with packets not being completed. I don't think the AF_XDP send loop or receive loop are at fault here. Nothing is backing up.

	I am able to send a significant number of packets from 10+ clients, 1000 packets per-second each to the server and it handles them fine.

	The only thing is that occasionally we see this: could not reserve packets in fill queue (0)

	It would be good to solve this problem, so at least we can be confident that the receive packet side is solid.

	Simplest approach to fixing the fill queue stall seems to work. Added a fatal error in case non-zero fail to reserve can happen.

	OK I'm pretty confident I know what's going on. The same frame pointer is being used for multiple packets, thus, when the packets all have the same size, it does send a stream of them, but when teh packets have different sizes, they rarely get through.

	This explains both why the random size packets (mostly) fail, and why it fails similarly when mulitple addresses are added to the mix, again, these packet data is re-used so the packets get all messed up.

	Solution: Verify this is the case by printing out the pointer for the packet data for each packet sent.

	Alternatively, something is messed up with the descriptor causing this to be all jacked up. TBD.

	OK. It's really simple. The send frame allocator is written and read from multiple threads, but it's not protected by a mutex.

	Thus the frames are getting trashed between the two threads...

	Should be able to verify this is the case by putting a simple mutex around the frame allocator.

	No. It is a threading issue, or it is an issue where frames are getting re-used incorrectly

	BUT

	It's not caused by lack of synchronization. Each send queue has its own umem and send/receive allocators, and these allocators and frames are only used by one thread.

	What's going on here is that frames are getting completed and modified, before the driver actually sends the packet.

	I'm certain of it.

	If we could modify the frames so it rotates through them, instead of re-using the same frames over and over, it'd be more resilient.

	Test this theory by not freeing completed send frames.

	If the theory is true, the clients should receive packets again, up till the point where the frames wrap around.

	Yes. Confirmed.	

TODO
	








	-------------------------------

	Things I can investigate:

		1. Wireshark the packets written by a non-XDP server vs. the packets written by the the AF_XDP server and see if there is any binary difference in them

		2. Debug why we get this after a while, it could be clue: "error: could not reserve packets in fill queue" (although this is in receive so is probably unrelated) (done, sends are still broken).

		3. There could be something wrong with how I am sending packets at the ethernet level (eg. gateway)

		4. There could be something wrong with how I am sending packets at the IP level (hashes, addresses etc...)

		5. I could run the server on my hulk linux, and see if the random sized packets get through across the ethernet switch. If they do, then the problem is probably at the IP or UDP header level?

		6. Restructure the server so it sends n packets per-client (for example 10), and doesn't do it in response to the server receiving packets, but sends them each frame. Does this change anything?

		7. I could change the client packets sent to the server and make them smaller (eg. 100 bytes fixed)

		8. I could disable timeout for clients and print out when the XDP server receives packets from clients. Make sure the server is actually able to receive packets from all clients. (done, it can receive...)

		9. It could just be that packets somehow don't get from the user app to the XDP program. There could be a logic error there, maybe timing related?

		10. Look across *everything* related to packet size passed in from teh application. Can it trip different codepaths?

		11. Force the packet size to 100 bytes in the XDP send thread, even though its passed in as random packet sizes from user. Disable client verify packet. Do the packets now get through?


